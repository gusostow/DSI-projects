{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 6\n",
    "\n",
    "\n",
    "## NLP and Machine Learning on [travel.statsexchange.com](http://travel.stackexchange.com/) data\n",
    "\n",
    "---\n",
    "\n",
    "In Project 7 you'll be doing NLP and machine learning on post data from stackexchange's travel subdomain. \n",
    "\n",
    "This project is setup like a mini Kaggle competition. You are given the training data and when projects are submitted your model will be tested on the held-out testing data. There will be prizes for the people who build models that perform best on the held out test set!\n",
    "\n",
    "---\n",
    "\n",
    "## Notes on the data\n",
    "\n",
    "The data is again compressed into the `.7z` file format to save space. There are 6 .csv files and one readme file that contains some information on the fields.\n",
    "\n",
    "    posts_train.csv\n",
    "    comments_train.csv\n",
    "    users.csv\n",
    "    badges.csv\n",
    "    votes_train.csv\n",
    "    tags.csv\n",
    "    readme.txt\n",
    "    \n",
    "The data is located in your datasets folder:\n",
    "\n",
    "    DSI-SF-2/datasets/stack_exchange_travel.7z\n",
    "    \n",
    "If you're interested in where this data came from and where to get more data from other stackexchange subdomains, see here:\n",
    "\n",
    "https://ia800500.us.archive.org/22/items/stackexchange/readme.txt\n",
    "\n",
    "\n",
    "### Recommended Utilities for .7z\n",
    "\n",
    "- For OSX [Keka](http://www.kekaosx.com/en/) or [The Unarchiver](http://wakaba.c3.cx/s/apps/unarchiver.html). \n",
    "- For Windows [7-zip](http://www.7-zip.org/) is the standard. \n",
    "- For Linux try the `p7zip` utility.  `sudo apt-get install p7zip`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 1. Use LDA to find what topics are discussed on travel.stackexchange.com.\n",
    "\n",
    "---\n",
    "\n",
    "Text can be found in the posts and the comments datasets. The `ParentId` column in the posts dataset indicates what the \"question\" post was for a given post. Comment text can be merged onto the post they are part of with the `PostId` field.\n",
    "\n",
    "The text may have some HTML tags. BeautifulSoup has convenient ways to get rid of markup or extract text if you need to. You can also parse the strings yourself if you like.\n",
    "\n",
    "The tags dataset has the \"tags\" that the users have officially given the post.\n",
    "\n",
    "**1.1 Implement LDA against the text features of the dataset(s).**\n",
    "\n",
    "- This can be posts or a combination of posts and comments if you want more power.\n",
    "- Find optimal **K/num_topics**.\n",
    "\n",
    "**1.2 Compare your topics to the tags. Do the LDA topics make sense? How do they compare to the tags?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = pd.read_csv(\"stack_exchange_travel/posts_train.csv\")\n",
    "comments = pd.read_csv(\"stack_exchange_travel/comments_train.csv\")\n",
    "tags = pd.read_csv(\"stack_exchange_travel/tags.csv\")\n",
    "votes = pd.read_csv(\"stack_exchange_travel/votes_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Id</th>\n",
       "      <th>PostId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>UserDisplayName</th>\n",
       "      <th>UserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-06-21T20:25:14.257</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>To help with the cruise line question: Where a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CreationDate  Id  PostId  Score  \\\n",
       "0  2011-06-21T20:25:14.257   1       1      0   \n",
       "\n",
       "                                                Text UserDisplayName  UserId  \n",
       "0  To help with the cruise line question: Where a...             NaN    12.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_array = posts[[\"Id\", \"Body\"]]\n",
    "comments_array = comments[[\"PostId\", \"Text\"]]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allcomments = pd.DataFrame(comments_array.groupby(\"PostId\")[\"Text\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>PostId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PostId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To help with the cruise line question: Where a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  PostId\n",
       "PostId                                                           \n",
       "1       To help with the cruise line question: Where a...       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allcomments[\"PostId\"] = allcomments.index\n",
    "allcomments.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "posts_array.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_comments = pd.merge(posts_array, allcomments, how = \"left\", left_on = \"Id\", right_on = \"PostId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Body</th>\n",
       "      <th>Text</th>\n",
       "      <th>PostId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;My fiancée and I are looking for a good Car...</td>\n",
       "      <td>To help with the cruise line question: Where a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Body  \\\n",
       "0   1  <p>My fiancée and I are looking for a good Car...   \n",
       "\n",
       "                                                Text  PostId  \n",
       "0  To help with the cruise line question: Where a...     1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_comments.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_arrays = zip(posts_comments[\"PostId\"].values,\n",
    "    posts_comments[\"Body\"].values,\n",
    "    posts_comments[\"Text\"].values)\n",
    "                \n",
    "\n",
    "fulltext = []\n",
    "\n",
    "for p_id, post, comment in text_arrays:\n",
    "    if np.isnan(p_id):\n",
    "        fulltext.append(post)\n",
    "    else:\n",
    "        fulltext.append(post + comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def removehtml(x):\n",
    "    soup = BeautifulSoup(x, \"lxml\")\n",
    "    return soup.get_text()\n",
    "\n",
    "cleantext = map(removehtml, fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\"id\": list(posts_array[\"Id\"].values),\n",
    "       \"text\": cleantext}\n",
    "\n",
    "finalframe = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#time to vectorize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(stop_words = \"english\", ngram_range=(1,2), \n",
    "                      max_df = 0.8, min_df = 0.03)\n",
    "\n",
    "vectorizer = vect.fit(finalframe[\"text\"].values)\n",
    "vectorized = vect.transform(finalframe[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 2. What makes an answer likely to be \"accepted\"?\n",
    "\n",
    "---\n",
    "\n",
    "**2.1 Build a model to predict whether a post will be marked as the answer.**\n",
    "\n",
    "- This is a classification problem.\n",
    "- You're free to use any of the machine learning algorithms or techniques we have learned in class to build the best model you can.\n",
    "- NLP will be very useful here for pulling out useful and relevant features from the data. \n",
    "- Though not required, using bagging and boosting models like Random Forests and Gradient Boosted Trees will _probably_ get you the highest performance on the test data (but who knows!).\n",
    "\n",
    "\n",
    "**2.2 Evaluate the performance of your classifier with a confusion matrix and accuracy. Explain how your model is performing.**\n",
    "\n",
    "**2.3 Plot either a ROC curve or precision-recall curve (or both!) and explain what they tell you about your model.**\n",
    "\n",
    "NOTE: You should only be predicting this for `PostTypeID=2` posts, which are the \"answer\" posts. This doesn't mean, however, that you can't or shouldn't use the parent questions as predictors!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accepted = list(posts[\"AcceptedAnswerId\"].unique())\n",
    "accepted = [i for i in accepted if not np.isnan(i)]\n",
    "posts[\"accepted\"] = np.zeros(posts.shape[0], dtype = np.int8)\n",
    "posts.loc[posts[\"Id\"].isin(accepted), \"accepted\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'AcceptedAnswerId', u'AnswerCount', u'Body', u'ClosedDate',\n",
       "       u'CommentCount', u'CommunityOwnedDate', u'CreationDate',\n",
       "       u'FavoriteCount', u'Id', u'LastActivityDate', u'LastEditDate',\n",
       "       u'LastEditorDisplayName', u'LastEditorUserId', u'OwnerDisplayName',\n",
       "       u'OwnerUserId', u'ParentId', u'PostTypeId', u'Score', u'Tags', u'Title',\n",
       "       u'ViewCount', u'accepted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23967, 2)\n",
      "(23967, 449)\n",
      "(23967,)\n"
     ]
    }
   ],
   "source": [
    "#remove posts with missing bodies\n",
    "posts = posts.loc[~ posts.Body.isnull(), :]\n",
    "\n",
    "y = posts.loc[posts.PostTypeId == 2, \"accepted\"]\n",
    "X = posts.loc[posts.PostTypeId == 2, [\"Score\", \"CommentCount\"]]\n",
    "\n",
    "corpus = posts.loc[posts.PostTypeId == 2, \"Body\"].values\n",
    "\n",
    "vectorized_answers = vect.transform(corpus)\n",
    "\n",
    "print X.values.shape\n",
    "print vectorized_answers.shape\n",
    "print y.shape\n",
    "\n",
    "X = np.hstack((X.values, vectorized_answers.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy:  0.735761672299\n"
     ]
    }
   ],
   "source": [
    "#time to fit some models\n",
    "\n",
    "#logistic regression baseline:\n",
    "logit = LogisticRegression()\n",
    "print \"baseline accuracy: \", np.mean(cross_val_score(logit, X, y, cv = 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost accuracy:  0.734092710811\n"
     ]
    }
   ],
   "source": [
    "#gradient boosted tree\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.5, 0.1],\n",
    "    \"max_depth\": [4, 8, 10],\n",
    "    \"subsample\": [0.5, 0.75, 1],\n",
    "    \"reg_lambda\": [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "grd = GridSearchCV(xgb, param_grid = param_grid, verbose=2, n_jobs = -1, cv = 2)\n",
    "\n",
    "print \"xgboost accuracy: \", np.mean(cross_val_score(xgb, X, y, cv = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 3. What is the score of a post?\n",
    "\n",
    "---\n",
    "\n",
    "**3.1 Build a model that predicts the score of a post.**\n",
    "\n",
    "- This is a regression problem now. \n",
    "- You can and should be predicting score for both \"question\" and \"answer\" posts, so keep them both in your dataset.\n",
    "- Again, use any techniques that you think will get you the best model.\n",
    "\n",
    "**3.2 Evaluate the performance of your model with cross-validation and report the results.**\n",
    "\n",
    "**3.3 What is important for determining the score of a post, if anything?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4. How many views does a post have?\n",
    "\n",
    "---\n",
    "\n",
    "**4.1 Build a model that predicts the number of views a post has.**\n",
    "\n",
    "- This is another regression problem. \n",
    "- Predict the views for all posts, not just the \"answer\" posts.\n",
    "\n",
    "**4.2 Evaluate the performance of your model with cross-validation and report the results.**\n",
    "\n",
    "**4.3 What is important for the number of views a post has, if anything?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 5. Build a pipeline or other code to automate evaluation of your models on the test data.\n",
    "\n",
    "---\n",
    "\n",
    "Now that you've constructed your three predictive models, build a pipeline or code that can easily load up the raw testing data and evaluate your models on it.\n",
    "\n",
    "The testing data that is held out is in the same raw format as the training data you have. _Any cleaning and preprocessing that you did on the training data will need to be done on the testing data as well!_\n",
    "\n",
    "This is a good opportunity to practice building pipelines, but you're not required to. Custom functions and classes are fine as long as they are able to process and test the new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 6. Lets Model - Tournament for stock market predictions\n",
    "\n",
    ">Start this section of the project by downloading the train and test datasets from the following site: https://numer.ai/rules\n",
    "\n",
    "> - The data set is clean, your goal is to develop a classification model(s) \n",
    "> - Report all the results including log loss, and other coefficients you consider iteresting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
