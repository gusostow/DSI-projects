{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 4: Classification with Yelp, SGD, OOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp\n",
    "\n",
    "---\n",
    "\n",
    "In this project you will be investigating a small version of the [Yelp challenge dataset](https://www.yelp.com/dataset_challenge). You'll practice using classification algorithms, cross-validation, gridsearching â€“ all that good stuff.\n",
    "\n",
    "---\n",
    "\n",
    "### The data\n",
    "\n",
    "There are 5 individual .csv files that have the information, zipped into .7z format like with the SF data last project. The dataset is located in your datasets folder:\n",
    "\n",
    "    DSI-SF-2/datasets/yelp_arizona_data.7z\n",
    "\n",
    "The columns in each are:\n",
    "\n",
    "    businesses_small_parsed.csv\n",
    "        business_id: unique business identifier\n",
    "        name: name of the business\n",
    "        review_count: number of reviews per business\n",
    "        city: city business resides in\n",
    "        stars: average rating\n",
    "        categories: categories the business falls into (can be one or multiple)\n",
    "        latitude\n",
    "        longitude\n",
    "        neighborhoods: neighborhoods business belongs to\n",
    "        variable: \"property\" of the business (a tag)\n",
    "        value: True/False for the property\n",
    "        \n",
    "    reviews_small_nlp_parsed.csv\n",
    "        user_id: unique user identifier\n",
    "        review_id: unique review identifier\n",
    "        votes.cool: how many thought the review was \"cool\"\n",
    "        business_id: unique business id the review is for\n",
    "        votes.funny: how many thought the review was funny\n",
    "        stars: rating given\n",
    "        date: date of review\n",
    "        votes.useful: how many thought the review was useful\n",
    "        ... 100 columns of counts of most common 2 word phrases that appear in reviews in this review\n",
    "        \n",
    "    users_small_parsed.csv\n",
    "        yelping_since: signup date\n",
    "        compliments.plain: # of compliments \"plain\"\n",
    "        review_count: # of reviews:\n",
    "        compliments.cute: total # of compliments \"cute\"\n",
    "        compliments.writer: # of compliments \"writer\"\n",
    "        compliments.note: # of compliments \"note\" (not sure what this is)\n",
    "        compliments.hot: # of compliments \"hot\" (?)\n",
    "        compliments.cool: # of compliments \"cool\"\n",
    "        compliments.profile: # of compliments \"profile\"\n",
    "        average_stars: average rating\n",
    "        compliments.more: # of compliments \"more\"\n",
    "        elite: years considered \"elite\"\n",
    "        name: user's name\n",
    "        user_id: unique user id\n",
    "        votes.cool: # of votes \"cool\"\n",
    "        compliments.list: # of compliments \"list\"\n",
    "        votes.funny: # of compliments \"funny\"\n",
    "        compliments.photos: # of compliments \"photos\"\n",
    "        compliments.funny: # of compliments \"funny\"\n",
    "        votes.useful: # of votes \"useful\"\n",
    "       \n",
    "    checkins_small_parsed.csv\n",
    "        business_id: unique business identifier\n",
    "        variable: day-time identifier of checkins (0-0 is Sunday 0:00 - 1:00am,  for example)\n",
    "        value: # of checkins at that time\n",
    "    \n",
    "    tips_small_nlp_parsed.csv\n",
    "        user_id: unique user identifier\n",
    "        business_id: unique business identifier\n",
    "        likes: likes that the tip has\n",
    "        date: date of tip\n",
    "        ... 100 columns of counts of most common 2 word phrases that appear in tips in this tip\n",
    "\n",
    "The reviews and tips datasets in particular have parsed \"NLP\" columns with counts of 2-word phrases in that review or tip (a \"tip\", it seems, is some kind of smaller review).\n",
    "\n",
    "The user dataset has a lot of columns of counts of different compliments and votes. I'm not sure whether the compliments or votes are _by_ the user or _for_ the user.\n",
    "\n",
    "---\n",
    "\n",
    "If you look at the website, or the full data, you'll see I have removed pieces of the data and cut it down quite a bit. This is to simplify it for this project. Specifically, business are limited to be in these cities:\n",
    "\n",
    "    Phoenix\n",
    "    Surprise\n",
    "    Las Vegas\n",
    "    Waterloo\n",
    "\n",
    "Apparently there is a city called \"Surprise\" in Arizona. \n",
    "\n",
    "Businesses are also restricted to at least be in one of the following categories, because I thought the mix of them was funny:\n",
    "\n",
    "    Airports\n",
    "    Breakfast & Brunch\n",
    "    Bubble Tea\n",
    "    Burgers\n",
    "    Bars\n",
    "    Bakeries\n",
    "    Breweries\n",
    "    Cafes\n",
    "    Candy Stores\n",
    "    Comedy Clubs\n",
    "    Courthouses\n",
    "    Dance Clubs\n",
    "    Fast Food\n",
    "    Museums\n",
    "    Tattoo\n",
    "    Vape Shops\n",
    "    Yoga\n",
    "    \n",
    "---\n",
    "\n",
    "### Project requirements\n",
    "\n",
    "**You will be performing 4 different sections of analysis, like in the last project.**\n",
    "\n",
    "Remember that classification targets are categorical and regression targets are continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, RidgeCV, LassoCV,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1. Constructing a \"profile\" for Las Vegas\n",
    "\n",
    "---\n",
    "\n",
    "Yelp is interested in building out what they are calling \"profiles\" for cities. They want you to start with just Las Vegas to see what a prototype of this would look like. Essentially, they want to know what makes Las Vegas distinct from the other four.\n",
    "\n",
    "Use the data you have to predict Las Vegas from the other variables you have. You should not be predicting the city from any kind of location data or other data perfectly associated with that city (or another city).\n",
    "\n",
    "You may use any classification algorithm you deem appropriate, or even multiple models. You should:\n",
    "\n",
    "1. Build at least one model predicting Las Vegas vs. the other cities.\n",
    "- Validate your model(s).\n",
    "- Interpret and visualize, in some way, the results.\n",
    "- Write up a \"profile\" for Las Vegas. This should be a writeup converting your findings from the model(s) into a human-readable description of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>city</th>\n",
       "      <th>stars</th>\n",
       "      <th>categories</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>neighborhoods</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>is_vegas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EmzaQR5hQlF0WIl24NxAZA</td>\n",
       "      <td>Sky Lounge</td>\n",
       "      <td>25</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2.5</td>\n",
       "      <td>['American (New)', 'Nightlife', 'Dance Clubs',...</td>\n",
       "      <td>33.448399</td>\n",
       "      <td>-112.071702</td>\n",
       "      <td>[]</td>\n",
       "      <td>attributes.Ambience.divey</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        name  review_count     city  stars  \\\n",
       "0  EmzaQR5hQlF0WIl24NxAZA  Sky Lounge            25  Phoenix    2.5   \n",
       "\n",
       "                                          categories   latitude   longitude  \\\n",
       "0  ['American (New)', 'Nightlife', 'Dance Clubs',...  33.448399 -112.071702   \n",
       "\n",
       "  neighborhoods                   variable  value is_vegas  \n",
       "0            []  attributes.Ambience.divey  False    False  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business = pd.read_csv(\"/Users/augustus/Desktop/DSI_notes_assignments/projects/project4/yelp_arizona_data/businesses_small_parsed.csv\")\n",
    "\n",
    "\n",
    "\n",
    "business[\"is_vegas\"] = business[\"city\"] == \"Las Vegas\"\n",
    "\n",
    "business.head()\n",
    "\n",
    "business.drop([\"latitude\", \"longitude\", \"neighborhoods\"], axis = 1)\n",
    "\n",
    "business.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4132, 10)\n",
      "(4132, 81)\n"
     ]
    }
   ],
   "source": [
    "#business_pivot = business.pivot(index = \"business_id\", columns = \"variable\", values = \"value\")\n",
    "\n",
    "#business = business.drop([\"variable\", \"value\"], axis = 1)\n",
    "\n",
    "business = business.drop_duplicates()\n",
    "\n",
    "print business.shape\n",
    "\n",
    "#business_pivot = business_pivot.reset_index()\n",
    "\n",
    "print business_pivot.shape\n",
    "\n",
    "business = pd.merge(business, business_pivot, how = \"inner\", on = \"business_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2. Different categories of ratings\n",
    "\n",
    "---\n",
    "\n",
    "Yelp is finally ready to admit that their rating system sucks. No one cares about the ratings, they just use the site to find out what's nearby. The ratings are simply too unreliable for people. \n",
    "\n",
    "Yelp hypothesizes that this is, in fact, because different people tend to give their ratings based on different things. They believe that perhaps some people always base their ratings on quality of food, others on service, and perhaps other categories as well. \n",
    "\n",
    "1. Do some users tend to talk about service more than others in reviews/tips? Divide up the tips/reviews into more \"service-focused\" ones and those less concerned with service.\n",
    "2. Create two new ratings for businesses: ratings from just the service-focused reviews and ratings from the non-service reviews.\n",
    "3. Construct a regression model for each of the two ratings. They should use the same predictor variables (of your choice). \n",
    "4. Validate the performance of the models.\n",
    "5. Do the models coefficients differ at all? What does this tell you about the hypothesis that there are in fact two different kinds of ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. Identifying \"elite\" users\n",
    "\n",
    "---\n",
    "\n",
    "Yelp, though having their own formula for determining whether a user is elite or not, is interested in delving deeper into what differentiates an elite user from a normal user at a broader level.\n",
    "\n",
    "Use a classification model to predict whether a user is elite or not. Note that users can be elite in some years and not in others.\n",
    "\n",
    "1. What things predict well whether a user is elite or not?\n",
    "- Validate the model.\n",
    "- If you were to remove the \"counts\" metrics for users (reviews, votes, compliments), what distinguishes an elite user, if anything? Validate the model and compare it to the one with the count variables.\n",
    "- Think of a way to visually represent your results in a compelling way.\n",
    "- Give a brief write-up of your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 4. Find something interesting on your own\n",
    "\n",
    "---\n",
    "\n",
    "You want to impress your superiors at Yelp by doing some investigation into the data on your own. You want to do classification, but you're not sure on what.\n",
    "\n",
    "1. Create a hypothesis or hypotheses about the data based on whatever you are interested in, as long as it is predicting a category of some kind (classification).\n",
    "2. Explore the data visually (ideally related to this hypothesis).\n",
    "3. Build one or more classification models to predict your target variable. **Your modeling should include gridsearching to find optimal model parameters.**\n",
    "4. Evaluate the performance of your model. Explain why your model may have chosen those specific parameters during the gridsearch process.\n",
    "5. Write up what the model tells you. Does it validate or invalidate your hypothesis? Write this up as if for a non-technical audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. ROC and Precision-recall\n",
    "\n",
    "---\n",
    "\n",
    "Some categories have fewer overall businesses than others. Choose two categories of businesses to predict, one that makes your proportion of target classes as even as possible, and another that has very few businesses and thus makes the target varible imbalanced.\n",
    "\n",
    "1. Create two classification models predicting these categories. Optimize the models and choose variables as you see fit.\n",
    "- Make confusion matrices for your models. Describe the confusion matrices and explain what they tell you about your models' performance.\n",
    "- Make ROC curves for both models. What do the ROC curves describe and what do they tell you about your model?\n",
    "- Make Precision-Recall curves for the models. What do they describe? How do they compare to the ROC curves?\n",
    "- Explain when Precision-Recall may be preferable to ROC. Is that the case in either of your models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## OOP, Stochastic Gradient Descent, and Linear Algebra\n",
    "\n",
    "---\n",
    "\n",
    "In this part of the project, you will implement machine learning algorithms we have covered in class from scratch. \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6. Extend the `LinearRegrssion` class to include the following: \n",
    "\n",
    "- Calculate confidence intervals at X% on the $\\beta$ coefficients and predicted y value when the model is not regularized. Here is a good overview of calculating confidence intervals: http://imgur.com/a/O1ZFk\n",
    "\n",
    "\n",
    "\n",
    "- An option to regularize the model with Ridge Regression. The method should automatically calculate the best `alpha` for the user. The closed form solution for Ridge Regression may be helpful [Hint: Think of the `n_alphas` parameter in sklearn]\n",
    "\n",
    "Protip: Compare if your implemenation works by applying it on the Diabetes Dataset and seeing it how it compares to opensource implemenations in `sklearn` and `statsmodels`. \n",
    "\n",
    "http://statsmodels.sourceforge.net/devel/examples/#regression\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 7. Extend the `NearestNeighbor` class such that the fit method doesn't use a `for loop`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 8. Implement `LogisticRegression` that fits the model using Stochastic Gradient Descent. \n",
    "\n",
    "Check if the results of your `LogisticRegression` implementation matches with the Sklearn implementation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "### 9. Singular Value Decomposition \n",
    "\n",
    "- Read articles on image compression with SVD and use `numpy` apply it to images from the MNIST Datasets. To get the MNIST Datset, use the following comands. Note that the `mnist` data has shape `(70000, 784)`. You will need to convert it to 28x28 images [Hint: `np.reshape`]. \n",
    "       \n",
    "       from sklearn.datasets import fetch_mldata\n",
    "       mnist = fetch_mldata('MNIST original', data_home=custom_data_home)\n",
    "\n",
    "    https://inst.eecs.berkeley.edu/~ee127a/book/login/l_svd_apps_image.html\n",
    "\n",
    "    http://www.frankcleary.com/svdimage/\n",
    "    \n",
    "    \n",
    "- Apply Logistic Regression to classify MNIST Images. This may require having the images be `784` dimensions rather than `28x28`. Compare results of the classifier on a test set when you apply it on the raw dataset vs. the compressed version of dataset from SVD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Interview Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Questions \n",
    "- Implement Binary Search in Python and write down the algorithmic complexity of the Binary Search Algorithm.\n",
    "\n",
    "\n",
    "- Implement Merge Sort in Python [Hint: First implement the `merge` function that combines two sorted arrays into one sorted array linear time]. What is the time complexity of `mergesort` ?\n",
    "\n",
    "\n",
    "- Find the closest pair from two sorted arrays. Given two sorted arrays and a number x, find the pair whose sum is closest to x and the pair has an element from each array. We are given two arrays ar1[0â€¦m-1] and ar2[0..n-1] and a number x, we need to find the pair ar1[i] + ar2[j] such that absolute value of (ar1[i] + ar2[j] â€“ x) is minimum. What is the time complexity of this algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binary search\n",
    "\n",
    "sample_list = [10, 16, 19, 22, 43, 98, 101]\n",
    "sample_num = 98\n",
    "\n",
    "def binary_search(num, search_list):\n",
    "    length = len(search_list)\n",
    "    min_index = 0\n",
    "    max_index = length - 1 \n",
    "    guess = 0\n",
    "    \n",
    "    while guess != num:\n",
    "        guess_index = (max_index + min_index) / 2\n",
    "        guess = search_list[guess_index]\n",
    "        if num > guess:\n",
    "            min_index = guess_index\n",
    "        elif num < guess:\n",
    "            max_index = guess_index\n",
    "        elif min_index == max_index:\n",
    "            return False\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        return guess_index\n",
    "    \n",
    "binary_search(sample_num, sample_list)\n",
    "\n",
    "\n",
    "#time complexity: log2(n)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-17221e311536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-17221e311536>\u001b[0m in \u001b[0;36msort\u001b[0;34m(input_list)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0msplit_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-114-17221e311536>\u001b[0m in \u001b[0;36msort\u001b[0;34m(input_list)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0msplit_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "#MergeSort\n",
    "\n",
    "sample_list_1 = range(20)\n",
    "sample_list_2 = map(lambda x: x* 3, range(23))\n",
    "\n",
    "def merge(list1, list2):\n",
    "        \n",
    "    output = []\n",
    "    \n",
    "    while len(list1) and len(list2) != 0:\n",
    "        if list1[0] > list2[0]:\n",
    "            output.append(list1[0])\n",
    "            list1 = list1[1:]\n",
    "        elif list2[0] > list1[0]:\n",
    "            output.append(list2[0])\n",
    "            list2 = list2[1:]\n",
    "        elif list1[0] == list2[0]:\n",
    "            output.append(list1[0])\n",
    "            output.append(list2[0])\n",
    "            list1 = list1[1:]\n",
    "            list2 = list2[1:]\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "split_list = []\n",
    "\n",
    "def sort(input_list):\n",
    "    length = len(input_list)\n",
    "    list1 = input_list[:length/2]\n",
    "    list2 = input_list[-length/2:]\n",
    "    \n",
    "    while \n",
    "\n",
    "test = list(np.random.randint(0, 9, size = 10))\n",
    "sort(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability  and Statistics Questions \n",
    "\n",
    "- In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the probability that you see at least one shooting star in the period of an hour?\n",
    "    - The probability of not seeing a shooting star in an hour is 0.8^4 = 0.4096\n",
    "    - So the probability of seeing at least one star is 1 - 0.4096 = 0.5904\n",
    "\n",
    "\n",
    "- A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?\n",
    "    - 0.5?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions Related to Material\n",
    "\n",
    "- *Explain what is bias-variance tradeoff to a non-technical person. Why is it a useful framework? How do you detect bias in your models? How about you variance? *\n",
    "\n",
    "    - Bias measures a model's ability to accurately make predictions given ideal information. \n",
    "    - Variance is how unpredictable a model is depending on which data it's trained on\n",
    "    - Useful because it helps you tune the sensitivity of your model. Is the model too sensitive to noise (high variance)? Is it complicated enough to capture the structure of the data (high bias).  \n",
    "    \n",
    "    \n",
    "- *Explain why cross-validation is important.* \n",
    "    - A single training set might not accurately reflect the features of a test set. To evaluate a model, the train / test process should be repeated many times to balance out any funky splits that might have happened due to chance. \n",
    "\n",
    "- *What is regularization? What are the advantages/disadvantages of different types of regularization schemes?* \n",
    "    - Regularization throttles over-fitting (keeps variance low). It is an automatic method to keep your model as simple (parsimonious as possible). \n",
    "    - For regressions there are three main regularization algorithms.\n",
    "        - Ridge: Minimizes l2 norm of the parameters.\n",
    "        - Lasso: Minimizes l1 norm of the parameters. More agressively throttles the size of coefficients than Ridge. \n",
    "        - ElasticNet: A combination of Ridge and Lasso. \n",
    "\n",
    "\n",
    "- *What are the loss functions associated with linear regression, logistic regression, and KNN?* \n",
    "    - Linear regression: The sum of squared difference between the true values and the predicted values. \n",
    "    - Logistic regression (binary): Sum of difference between true outcome and its predicted probability (?).   \n",
    "\n",
    "\n",
    "- *Can you give examples of feature engineering on datasets you have worked with?* \n",
    "    - In the iowa liquor dataset, I made courser categories for liquor type. \n",
    "    - Add a quadratic term to a linear regression when the relationship doesn't seem linear in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
